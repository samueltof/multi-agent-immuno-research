import logging
from langchain_community.tools.file_management import WriteFileTool
from .decorators import create_logged_tool
from langchain.tools import tool

logger = logging.getLogger(__name__)

# Initialize file management tool with logging
LoggedWriteFile = create_logged_tool(WriteFileTool)
write_file_tool = LoggedWriteFile()

@tool
def read_csv_file(file_path: str, max_rows: int = None) -> str:
    """Read a CSV file and return its contents as a pandas DataFrame summary.
    
    This tool is designed to load datasets saved by other agents for analysis.
    
    Args:
        file_path: Path to the CSV file to read.
        max_rows: Maximum number of rows to load (None for all rows).
        
    Returns:
        A string containing information about the loaded dataset and a preview.
    """
    try:
        import pandas as pd
        import os
        
        # Check if file exists
        if not os.path.exists(file_path):
            return f"Error: File '{file_path}' does not exist."
        
        # Read the CSV file
        if max_rows:
            df = pd.read_csv(file_path, nrows=max_rows)
            load_info = f"(loaded first {max_rows} rows)"
        else:
            df = pd.read_csv(file_path)
            load_info = "(loaded all rows)"
        
        if df.empty:
            return f"CSV file '{file_path}' is empty."
        
        # Get basic information about the dataset
        info_str = f"""CSV file loaded successfully {load_info}:

Dataset Information:
- File: {file_path}
- Shape: {df.shape[0]} rows × {df.shape[1]} columns
- Columns: {', '.join(df.columns.tolist())}
- Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB

Data Types:
{df.dtypes.to_string()}

First 5 rows:
{df.head().to_string(index=False)}

Dataset loaded and ready for analysis. You can now access the data using pandas operations."""
        
        return info_str
        
    except Exception as e:
        return f"Error reading CSV file '{file_path}': {str(e)}"


@tool 
def load_csv_as_dataframe(file_path: str) -> str:
    """Load a CSV file as a pandas DataFrame for statistical analysis.
    
    This tool loads the full dataset and makes it available for analysis.
    Use this when you need to perform calculations on the complete dataset.
    
    Args:
        file_path: Path to the CSV file to load.
        
    Returns:
        A confirmation message. The dataset will be available as 'df' variable.
    """
    try:
        import pandas as pd
        import os
        
        # Check if file exists
        if not os.path.exists(file_path):
            return f"Error: File '{file_path}' does not exist."
        
        # Load the CSV file
        df = pd.read_csv(file_path)
        
        if df.empty:
            return f"Error: CSV file '{file_path}' is empty."
        
        # Make the dataframe available globally (this is a limitation of the tool system)
        # Instead, we'll return the data loading code that the agent can execute
        code_to_execute = f"""
import pandas as pd

# Load the dataset
df = pd.read_csv('{file_path}')
print(f"Dataset loaded: {{df.shape[0]}} rows × {{df.shape[1]}} columns")
print(f"Columns: {{', '.join(df.columns.tolist())}}")
print("\\nFirst 5 rows:")
print(df.head())
"""
        
        return f"""Dataset loading code prepared. Execute the following code to load the data:

```python
{code_to_execute.strip()}
```

This will load the complete dataset from '{file_path}' into a pandas DataFrame named 'df' for your analysis."""
        
    except Exception as e:
        return f"Error preparing dataset loading: {str(e)}"


@tool
def extract_file_paths_from_conversation(conversation_context: str = "") -> str:
    """Extract CSV file paths mentioned in the conversation history.
    
    This tool searches through the conversation for file paths generated by the data analyst
    and returns them in order of creation, which is useful when multiple datasets are created
    in sequence.
    
    Args:
        conversation_context: String containing conversation history or a description
                              of what files to look for (e.g., "epitope data", "species data")
        
    Returns:
        A string listing found file paths with descriptions.
    """
    try:
        import re
        import os
        from pathlib import Path
        
        # Look for file path patterns in the conversation context
        file_patterns = [
            r'saved to (outputs/[^\\s]+\\.csv)',
            r'File: (outputs/[^\\s]+\\.csv)',
            r'File Path: (outputs/[^\\s]+\\.csv)',
            r'CSV file: (outputs/[^\\s]+\\.csv)',
            r'(outputs/query_results_[^\\s]+\\.csv)',
            r'filename = [\'"]([^\'\"]+\\.csv)[\'"]',
        ]
        
        found_files = []
        
        for pattern in file_patterns:
            matches = re.findall(pattern, conversation_context, re.IGNORECASE)
            for match in matches:
                file_path = match.strip()
                if os.path.exists(file_path) and file_path not in found_files:
                    found_files.append(file_path)
        
        if not found_files:
            # If no files found in provided context, search the outputs directory
            outputs_dir = Path("outputs")
            if outputs_dir.exists():
                csv_files = list(outputs_dir.glob("*.csv"))
                # Sort by modification time (newest first)
                csv_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                found_files = [str(f) for f in csv_files[:10]]  # Get last 10 files
        
        if not found_files:
            return "No CSV files found in conversation or outputs directory."
        
        # Generate descriptions based on file names and provide file information
        result = "Found CSV files from conversation:\\n\\n"
        
        for i, file_path in enumerate(found_files, 1):
            file_name = Path(file_path).name
            
            # Try to infer content from filename
            content_desc = "data analysis"
            if "antigen_species" in file_name or "species" in file_name:
                content_desc = "antigen species distribution"
            elif "epitope" in file_name:
                content_desc = "epitope distribution"
            elif "query_results" in file_name:
                content_desc = "query results"
            
            # Get file size and modification time
            try:
                file_stat = os.stat(file_path)
                file_size = file_stat.st_size
                from datetime import datetime
                mod_time = datetime.fromtimestamp(file_stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S')
                
                result += f"{i}. **{file_path}**\\n"
                result += f"   - Content: {content_desc}\\n"
                result += f"   - Size: {file_size} bytes\\n"
                result += f"   - Modified: {mod_time}\\n\\n"
            except:
                result += f"{i}. **{file_path}** ({content_desc})\\n\\n"
        
        result += "**Usage:** Use `read_csv_file(file_path)` to examine the contents or `load_csv_as_dataframe(file_path)` to get code for loading the data."
        
        return result
        
    except Exception as e:
        return f"Error extracting file paths: {str(e)}"


